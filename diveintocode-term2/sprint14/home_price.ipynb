{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/DIC/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/anaconda3/envs/DIC/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/anaconda3/envs/DIC/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/envs/DIC/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "dataset_path =\"train.csv\"\n",
    "column_names = ['SalePrice', 'GrLivArea', 'YearBuilt']\n",
    "dataset = pd.read_csv(dataset_path, usecols=column_names)\n",
    "#学習データ準備\n",
    "ss1 = StandardScaler()\n",
    "ss2 = StandardScaler()\n",
    "y = dataset['SalePrice']\n",
    "X = dataset.loc[:, ['GrLivArea', 'YearBuilt']]\n",
    "# y = np.array(y)\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "y = ss1.fit_transform(y)\n",
    "\n",
    "# X = np.array(X)\n",
    "X = ss2.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/y-shiraishi/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch 0 loss : 595.8972  val_loss : 529.3991  r2 : -607.2555\n",
      "Epoch 1 loss : 40.210735  val_loss : 43.128487  r2 : -48.55267\n",
      "Epoch 2 loss : 25.887337  val_loss : 19.75308  r2 : -21.69539\n",
      "Epoch 3 loss : 6.275394  val_loss : 5.0515738  r2 : -4.8040285\n",
      "Epoch 4 loss : 4.130791  val_loss : 3.5779943  r2 : -3.1109529\n",
      "Epoch 5 loss : 3.0696335  val_loss : 2.1925857  r2 : -1.5191815\n",
      "Epoch 6 loss : 2.585132  val_loss : 1.8074689  r2 : -1.0766997\n",
      "Epoch 7 loss : 2.032832  val_loss : 1.4622049  r2 : -0.680007\n",
      "Epoch 8 loss : 1.7417351  val_loss : 1.2835605  r2 : -0.47475266\n",
      "Epoch 9 loss : 1.5650338  val_loss : 1.1898836  r2 : -0.36712217\n",
      "Epoch 10 loss : 1.3578842  val_loss : 1.1102562  r2 : -0.27563393\n",
      "Epoch 11 loss : 1.2550815  val_loss : 1.0518358  r2 : -0.20851147\n",
      "Epoch 12 loss : 1.1399052  val_loss : 1.0101993  r2 : -0.16067314\n",
      "Epoch 13 loss : 1.1019696  val_loss : 0.96313  r2 : -0.106592655\n",
      "Epoch 14 loss : 0.9979415  val_loss : 0.92397684  r2 : -0.06160748\n",
      "Epoch 15 loss : 0.9313279  val_loss : 0.8650296  r2 : 0.006120324\n",
      "Epoch 16 loss : 0.8254173  val_loss : 0.83726436  r2 : 0.038021266\n",
      "Epoch 17 loss : 0.7993373  val_loss : 0.78685105  r2 : 0.09594393\n",
      "Epoch 18 loss : 0.7526659  val_loss : 0.7817574  r2 : 0.10179627\n",
      "Epoch 19 loss : 0.67918867  val_loss : 0.7422153  r2 : 0.14722836\n",
      "Epoch 20 loss : 0.6798347  val_loss : 0.7090067  r2 : 0.1853835\n",
      "Epoch 21 loss : 0.59834015  val_loss : 0.6952515  r2 : 0.20118761\n",
      "Epoch 22 loss : 0.5506331  val_loss : 0.68082744  r2 : 0.21776026\n",
      "Epoch 23 loss : 0.5217556  val_loss : 0.66079444  r2 : 0.2407772\n",
      "Epoch 24 loss : 0.5186718  val_loss : 0.65395707  r2 : 0.24863303\n",
      "Epoch 25 loss : 0.6029029  val_loss : 0.66309196  r2 : 0.23813748\n",
      "Epoch 26 loss : 0.52419215  val_loss : 0.617985  r2 : 0.2899633\n",
      "Epoch 27 loss : 0.4792537  val_loss : 0.62014073  r2 : 0.2874865\n",
      "Epoch 28 loss : 0.45281908  val_loss : 0.5734444  r2 : 0.34113842\n",
      "Epoch 29 loss : 0.48651224  val_loss : 0.59110457  r2 : 0.32084775\n",
      "Epoch 30 loss : 0.4684084  val_loss : 0.56372947  r2 : 0.35230047\n",
      "Epoch 31 loss : 0.4451886  val_loss : 0.56928813  r2 : 0.34591377\n",
      "Epoch 32 loss : 0.47995  val_loss : 0.60098875  r2 : 0.30949128\n",
      "Epoch 33 loss : 0.44473767  val_loss : 0.56159306  r2 : 0.35475504\n",
      "Epoch 34 loss : 0.41244742  val_loss : 0.5592296  r2 : 0.35747057\n",
      "Epoch 35 loss : 0.4224502  val_loss : 0.5506757  r2 : 0.36729866\n",
      "Epoch 36 loss : 0.48135948  val_loss : 0.59267074  r2 : 0.3190483\n",
      "Epoch 37 loss : 0.48957175  val_loss : 0.5831918  r2 : 0.32993907\n",
      "Epoch 38 loss : 0.41919383  val_loss : 0.5441464  r2 : 0.3748005\n",
      "Epoch 39 loss : 0.3875299  val_loss : 0.53886503  r2 : 0.38086855\n",
      "Epoch 40 loss : 0.4008833  val_loss : 0.5569497  r2 : 0.36009014\n",
      "Epoch 41 loss : 0.40779224  val_loss : 0.5678247  r2 : 0.3475952\n",
      "Epoch 42 loss : 0.4409657  val_loss : 0.59620726  r2 : 0.31498498\n",
      "Epoch 43 loss : 0.40885615  val_loss : 0.5616847  r2 : 0.35464978\n",
      "Epoch 44 loss : 0.38752905  val_loss : 0.5528985  r2 : 0.36474472\n",
      "Epoch 45 loss : 0.37972453  val_loss : 0.54809266  r2 : 0.37026644\n",
      "Epoch 46 loss : 0.38305974  val_loss : 0.54674596  r2 : 0.3718137\n",
      "Epoch 47 loss : 0.383109  val_loss : 0.5503575  r2 : 0.36766422\n",
      "Epoch 48 loss : 0.3909099  val_loss : 0.5513693  r2 : 0.36650175\n",
      "Epoch 49 loss : 0.44342414  val_loss : 0.56182647  r2 : 0.35448694\n",
      "Epoch 50 loss : 0.50800604  val_loss : 0.61277163  r2 : 0.2959532\n",
      "Epoch 51 loss : 0.48131207  val_loss : 0.62321216  r2 : 0.28395754\n",
      "Epoch 52 loss : 0.4629554  val_loss : 0.6282931  r2 : 0.2781198\n",
      "Epoch 53 loss : 0.449896  val_loss : 0.60331684  r2 : 0.30681634\n",
      "Epoch 54 loss : 0.4512055  val_loss : 0.5972886  r2 : 0.31374252\n",
      "Epoch 55 loss : 0.44740865  val_loss : 0.5931444  r2 : 0.31850404\n",
      "Epoch 56 loss : 0.4437823  val_loss : 0.5915775  r2 : 0.3203044\n",
      "Epoch 57 loss : 0.44835174  val_loss : 0.5867705  r2 : 0.32582736\n",
      "Epoch 58 loss : 0.4565546  val_loss : 0.59304386  r2 : 0.31861955\n",
      "Epoch 59 loss : 0.46046603  val_loss : 0.59750515  r2 : 0.31349373\n",
      "Epoch 60 loss : 0.46646726  val_loss : 0.6077276  r2 : 0.30174863\n",
      "Epoch 61 loss : 0.4682017  val_loss : 0.6050708  r2 : 0.3048011\n",
      "Epoch 62 loss : 0.47632375  val_loss : 0.62273085  r2 : 0.2845105\n",
      "Epoch 63 loss : 0.4603116  val_loss : 0.6247264  r2 : 0.28221774\n",
      "Epoch 64 loss : 0.48065186  val_loss : 0.6269727  r2 : 0.27963686\n",
      "Epoch 65 loss : 0.47987518  val_loss : 0.621877  r2 : 0.28549153\n",
      "Epoch 66 loss : 0.47876492  val_loss : 0.61728406  r2 : 0.29076862\n",
      "Epoch 67 loss : 0.49648818  val_loss : 0.626607  r2 : 0.280057\n",
      "Epoch 68 loss : 0.49815732  val_loss : 0.6195766  r2 : 0.2881347\n",
      "Epoch 69 loss : 0.4968692  val_loss : 0.61319625  r2 : 0.29546535\n",
      "Epoch 70 loss : 0.491174  val_loss : 0.6151439  r2 : 0.29322767\n",
      "Epoch 71 loss : 0.49599147  val_loss : 0.6168035  r2 : 0.29132074\n",
      "Epoch 72 loss : 0.48652244  val_loss : 0.6029141  r2 : 0.3072791\n",
      "Epoch 73 loss : 0.4774308  val_loss : 0.59048283  r2 : 0.32156205\n",
      "Epoch 74 loss : 0.4697905  val_loss : 0.57970315  r2 : 0.33394742\n",
      "Epoch 75 loss : 0.4621089  val_loss : 0.5670865  r2 : 0.3484434\n",
      "Epoch 76 loss : 0.46255776  val_loss : 0.55588585  r2 : 0.36131245\n",
      "Epoch 77 loss : 0.45364103  val_loss : 0.540332  r2 : 0.37918305\n",
      "Epoch 78 loss : 0.44789082  val_loss : 0.5282345  r2 : 0.39308256\n",
      "Epoch 79 loss : 0.42519343  val_loss : 0.50900316  r2 : 0.41517848\n",
      "Epoch 80 loss : 0.42037985  val_loss : 0.5019612  r2 : 0.42326945\n",
      "Epoch 81 loss : 0.41384342  val_loss : 0.48984265  r2 : 0.43719304\n",
      "Epoch 82 loss : 0.42080188  val_loss : 0.48874527  r2 : 0.4384539\n",
      "Epoch 83 loss : 0.40759504  val_loss : 0.47371918  r2 : 0.45571816\n",
      "Epoch 84 loss : 0.39418757  val_loss : 0.46642646  r2 : 0.4640972\n",
      "Epoch 85 loss : 0.38907593  val_loss : 0.44837835  r2 : 0.48483366\n",
      "Epoch 86 loss : 0.38548017  val_loss : 0.4450305  r2 : 0.48868018\n",
      "Epoch 87 loss : 0.3830245  val_loss : 0.43636766  r2 : 0.49863338\n",
      "Epoch 88 loss : 0.385109  val_loss : 0.43006203  r2 : 0.50587827\n",
      "Epoch 89 loss : 0.36520833  val_loss : 0.41507652  r2 : 0.52309597\n",
      "Epoch 90 loss : 0.36681914  val_loss : 0.41127175  r2 : 0.5274675\n",
      "Epoch 91 loss : 0.36403134  val_loss : 0.40878722  r2 : 0.5303221\n",
      "Epoch 92 loss : 0.35453153  val_loss : 0.3990726  r2 : 0.54148376\n",
      "Epoch 93 loss : 0.3461891  val_loss : 0.38634944  r2 : 0.55610204\n",
      "Epoch 94 loss : 0.34412962  val_loss : 0.3848899  r2 : 0.55777895\n",
      "Epoch 95 loss : 0.3470315  val_loss : 0.3861248  r2 : 0.5563601\n",
      "Epoch 96 loss : 0.34104964  val_loss : 0.38255936  r2 : 0.56045663\n",
      "Epoch 97 loss : 0.3500502  val_loss : 0.38121715  r2 : 0.56199884\n",
      "Epoch 98 loss : 0.33602214  val_loss : 0.3752749  r2 : 0.5688262\n",
      "Epoch 99 loss : 0.3423887  val_loss : 0.37594557  r2 : 0.5680556\n",
      "Epoch 100 loss : 0.35170582  val_loss : 0.37867576  r2 : 0.56491876\n",
      "Epoch 101 loss : 0.34601358  val_loss : 0.37411392  r2 : 0.57016015\n",
      "Epoch 102 loss : 0.35273245  val_loss : 0.37717625  r2 : 0.56664157\n",
      "Epoch 103 loss : 0.3553061  val_loss : 0.3783212  r2 : 0.5653261\n",
      "Epoch 104 loss : 0.3523114  val_loss : 0.3720266  r2 : 0.57255834\n",
      "Epoch 105 loss : 0.3539999  val_loss : 0.36977673  r2 : 0.57514334\n",
      "Epoch 106 loss : 0.36643067  val_loss : 0.37650445  r2 : 0.56741345\n",
      "Epoch 107 loss : 0.35154027  val_loss : 0.36988991  r2 : 0.5750133\n",
      "Epoch 108 loss : 0.34349397  val_loss : 0.36006382  r2 : 0.586303\n",
      "Epoch 109 loss : 0.36049742  val_loss : 0.36809382  r2 : 0.5770769\n",
      "Epoch 110 loss : 0.37078404  val_loss : 0.3737895  r2 : 0.57053286\n",
      "Epoch 111 loss : 0.356327  val_loss : 0.36602804  r2 : 0.5794504\n",
      "Epoch 112 loss : 0.36621025  val_loss : 0.36683849  r2 : 0.5785192\n",
      "Epoch 113 loss : 0.37117118  val_loss : 0.3729245  r2 : 0.57152665\n",
      "Epoch 114 loss : 0.36300227  val_loss : 0.36421236  r2 : 0.58153653\n",
      "Epoch 115 loss : 0.39938617  val_loss : 0.38936433  r2 : 0.55263805\n",
      "Epoch 116 loss : 0.41149738  val_loss : 0.4044366  r2 : 0.53532076\n",
      "Epoch 117 loss : 0.42496198  val_loss : 0.40984684  r2 : 0.5291046\n",
      "Epoch 118 loss : 0.44261625  val_loss : 0.42757857  r2 : 0.50873166\n",
      "Epoch 119 loss : 0.48766536  val_loss : 0.46287498  r2 : 0.46817768\n",
      "Epoch 120 loss : 0.51206845  val_loss : 0.49137497  r2 : 0.4354325\n",
      "Epoch 121 loss : 0.53140247  val_loss : 0.52230895  r2 : 0.39989078\n",
      "Epoch 122 loss : 0.5375018  val_loss : 0.531792  r2 : 0.3889951\n",
      "Epoch 123 loss : 0.61685693  val_loss : 0.5974356  r2 : 0.31357366\n",
      "Epoch 124 loss : 0.63598776  val_loss : 0.6462631  r2 : 0.25747305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125 loss : 0.67377245  val_loss : 0.6697494  r2 : 0.23048836\n",
      "Epoch 126 loss : 0.7469716  val_loss : 0.7585499  r2 : 0.12846059\n",
      "Epoch 127 loss : 0.739903  val_loss : 0.7746117  r2 : 0.11000633\n",
      "Epoch 128 loss : 0.8767397  val_loss : 0.84733564  r2 : 0.02644986\n",
      "Epoch 129 loss : 0.86764205  val_loss : 0.84640163  r2 : 0.027522981\n",
      "Epoch 130 loss : 0.82990134  val_loss : 0.8552145  r2 : 0.017397404\n",
      "Epoch 131 loss : 0.83260113  val_loss : 0.8802513  r2 : -0.0113687515\n",
      "Epoch 132 loss : 0.9108151  val_loss : 0.96857786  r2 : -0.11285198\n",
      "Epoch 133 loss : 0.93378496  val_loss : 1.0148022  r2 : -0.16596174\n",
      "Epoch 134 loss : 0.94544995  val_loss : 1.0448357  r2 : -0.20046878\n",
      "Epoch 135 loss : 1.023091  val_loss : 1.1357915  r2 : -0.30497277\n",
      "Epoch 136 loss : 1.0060526  val_loss : 1.0776895  r2 : -0.23821628\n",
      "Epoch 137 loss : 1.1120092  val_loss : 1.1547315  r2 : -0.32673407\n",
      "Epoch 138 loss : 0.81428784  val_loss : 0.91577566  r2 : -0.05218458\n",
      "Epoch 139 loss : 1.1766933  val_loss : 1.244726  r2 : -0.43013358\n",
      "Epoch 140 loss : 0.8755275  val_loss : 1.0287529  r2 : -0.18199039\n",
      "Epoch 141 loss : 1.2041568  val_loss : 1.2674452  r2 : -0.45623696\n",
      "Epoch 142 loss : 0.967842  val_loss : 1.0728523  r2 : -0.2326585\n",
      "Epoch 143 loss : 0.6880347  val_loss : 0.78472203  r2 : 0.09839004\n",
      "Epoch 144 loss : 1.0836976  val_loss : 1.1633582  r2 : -0.33664572\n",
      "Epoch 145 loss : 0.66096014  val_loss : 0.8248535  r2 : 0.052280843\n",
      "Epoch 146 loss : 1.0485841  val_loss : 1.108946  r2 : -0.27412844\n",
      "Epoch 147 loss : 0.75335073  val_loss : 0.8605838  r2 : 0.011228323\n",
      "Epoch 148 loss : 0.62229985  val_loss : 0.715737  r2 : 0.17765075\n",
      "Epoch 149 loss : 0.9026912  val_loss : 0.9919051  r2 : -0.13965392\n",
      "Epoch 150 loss : 0.52634543  val_loss : 0.66702163  r2 : 0.23362243\n",
      "Epoch 151 loss : 0.88085604  val_loss : 0.9258629  r2 : -0.06377435\n",
      "Epoch 152 loss : 0.5296848  val_loss : 0.6130852  r2 : 0.2955929\n",
      "Epoch 153 loss : 0.6307681  val_loss : 0.7221082  r2 : 0.17033052\n",
      "Epoch 154 loss : 0.63696384  val_loss : 0.7312613  r2 : 0.159814\n",
      "Epoch 155 loss : 0.37294134  val_loss : 0.49192294  r2 : 0.4348029\n",
      "Epoch 156 loss : 0.87473184  val_loss : 0.93330497  r2 : -0.07232499\n",
      "Epoch 157 loss : 0.23521519  val_loss : 0.3582731  r2 : 0.58836055\n",
      "Epoch 158 loss : 1.0733721  val_loss : 1.0915202  r2 : -0.25410712\n",
      "Epoch 159 loss : 0.29892325  val_loss : 0.48069566  r2 : 0.44770253\n",
      "Epoch 160 loss : 0.53391814  val_loss : 0.58780617  r2 : 0.3246374\n",
      "Epoch 161 loss : 0.8952034  val_loss : 0.9133007  r2 : -0.049340963\n",
      "Epoch 162 loss : 0.266708  val_loss : 0.32537317  r2 : 0.626161\n",
      "Epoch 163 loss : 2.4791694  val_loss : 2.3228922  r2 : -1.6688976\n",
      "Epoch 164 loss : 0.9598263  val_loss : 1.2545707  r2 : -0.44144475\n",
      "Epoch 165 loss : 3.324137  val_loss : 3.6000566  r2 : -3.1363015\n",
      "Epoch 166 loss : 0.87588984  val_loss : 0.8694328  r2 : 0.0010612011\n",
      "Epoch 167 loss : 1.1933283  val_loss : 1.3727796  r2 : -0.57726145\n",
      "Epoch 168 loss : 0.29499936  val_loss : 0.53108174  r2 : 0.38981122\n",
      "Epoch 169 loss : 1.3133097  val_loss : 1.2562908  r2 : -0.443421\n",
      "Epoch 170 loss : 0.31017214  val_loss : 0.4491944  r2 : 0.48389608\n",
      "Epoch 171 loss : 1.0004363  val_loss : 1.0065068  r2 : -0.1564306\n",
      "Epoch 172 loss : 0.2638539  val_loss : 0.39439002  r2 : 0.5468638\n",
      "Epoch 173 loss : 0.8631288  val_loss : 0.87821066  r2 : -0.009024143\n",
      "Epoch 174 loss : 0.2252856  val_loss : 0.36232555  r2 : 0.5837044\n",
      "Epoch 175 loss : 0.7087938  val_loss : 0.7773289  r2 : 0.10688442\n",
      "Epoch 176 loss : 0.21287017  val_loss : 0.3428338  r2 : 0.60609955\n",
      "Epoch 177 loss : 0.65002644  val_loss : 0.72374254  r2 : 0.16845268\n",
      "Epoch 178 loss : 0.20374073  val_loss : 0.33096012  r2 : 0.61974186\n",
      "Epoch 179 loss : 0.6501734  val_loss : 0.72437084  r2 : 0.16773081\n",
      "Epoch 180 loss : 0.19992948  val_loss : 0.32518798  r2 : 0.62637377\n",
      "Epoch 181 loss : 0.7467556  val_loss : 0.8368548  r2 : 0.038491845\n",
      "Epoch 182 loss : 0.2289606  val_loss : 0.34614778  r2 : 0.60229194\n",
      "Epoch 183 loss : 0.9494555  val_loss : 1.062548  r2 : -0.22081935\n",
      "Epoch 184 loss : 0.3279948  val_loss : 0.43268594  r2 : 0.5028635\n",
      "Epoch 185 loss : 1.4319987  val_loss : 1.5363368  r2 : -0.7651812\n",
      "Epoch 186 loss : 0.48498496  val_loss : 0.5928454  r2 : 0.3188476\n",
      "Epoch 187 loss : 1.6383331  val_loss : 1.7434518  r2 : -1.0031471\n",
      "Epoch 188 loss : 0.34757048  val_loss : 0.47767457  r2 : 0.4511736\n",
      "Epoch 189 loss : 1.2300913  val_loss : 1.3433386  r2 : -0.5434351\n",
      "Epoch 190 loss : 0.23152876  val_loss : 0.3903746  r2 : 0.5514773\n",
      "Epoch 191 loss : 0.74104756  val_loss : 0.8790147  r2 : -0.009947896\n",
      "Epoch 192 loss : 0.24472201  val_loss : 0.39587027  r2 : 0.54516304\n",
      "Epoch 193 loss : 0.57196546  val_loss : 0.716457  r2 : 0.17682344\n",
      "Epoch 194 loss : 0.27560768  val_loss : 0.42157063  r2 : 0.5156345\n",
      "Epoch 195 loss : 0.48301193  val_loss : 0.6152646  r2 : 0.29308897\n",
      "Epoch 196 loss : 0.279241  val_loss : 0.41845414  r2 : 0.5192152\n",
      "Epoch 197 loss : 0.41652077  val_loss : 0.541428  r2 : 0.37792385\n",
      "Epoch 198 loss : 0.2678983  val_loss : 0.4030328  r2 : 0.53693366\n",
      "Epoch 199 loss : 0.37858915  val_loss : 0.502055  r2 : 0.42316163\n",
      "Epoch 200 loss : 0.26722887  val_loss : 0.38732785  r2 : 0.5549779\n",
      "Epoch 201 loss : 0.36118302  val_loss : 0.48262542  r2 : 0.4454853\n",
      "Epoch 202 loss : 0.28176293  val_loss : 0.40753084  r2 : 0.5317656\n",
      "Epoch 203 loss : 0.35572892  val_loss : 0.482134  r2 : 0.44604993\n",
      "Epoch 204 loss : 0.27880532  val_loss : 0.44294605  r2 : 0.4910751\n",
      "Epoch 205 loss : 0.4778642  val_loss : 0.62257785  r2 : 0.2846864\n",
      "Epoch 206 loss : 0.26038736  val_loss : 0.41983843  r2 : 0.51762474\n",
      "Epoch 207 loss : 0.7058688  val_loss : 0.8443567  r2 : 0.029872537\n",
      "Epoch 208 loss : 0.39610255  val_loss : 0.53510386  r2 : 0.38519\n",
      "Epoch 209 loss : 1.3456932  val_loss : 1.4679544  r2 : -0.68661296\n",
      "Epoch 210 loss : 0.7189256  val_loss : 0.8156526  r2 : 0.062852204\n",
      "Epoch 211 loss : 2.037589  val_loss : 2.0790431  r2 : -1.3887262\n",
      "Epoch 212 loss : 0.5101392  val_loss : 0.59148103  r2 : 0.3204152\n",
      "Epoch 213 loss : 1.1419333  val_loss : 1.2476088  r2 : -0.4334458\n",
      "Epoch 214 loss : 0.4356549  val_loss : 0.63422364  r2 : 0.27130586\n",
      "Epoch 215 loss : 0.653966  val_loss : 0.8389448  r2 : 0.036090553\n",
      "Epoch 216 loss : 0.5512233  val_loss : 0.7388672  r2 : 0.15107512\n",
      "Epoch 217 loss : 0.64028496  val_loss : 0.833999  r2 : 0.04177308\n",
      "Epoch 218 loss : 0.6330174  val_loss : 0.8399929  r2 : 0.03488636\n",
      "Epoch 219 loss : 0.65097755  val_loss : 0.88424367  r2 : -0.015955806\n",
      "Epoch 220 loss : 0.7188217  val_loss : 0.9674607  r2 : -0.11156833\n",
      "Epoch 221 loss : 0.72710675  val_loss : 0.9802281  r2 : -0.12623763\n",
      "Epoch 222 loss : 0.82319665  val_loss : 1.0858938  r2 : -0.24764252\n",
      "Epoch 223 loss : 0.77955526  val_loss : 1.0649384  r2 : -0.22356594\n",
      "Epoch 224 loss : 0.92562526  val_loss : 1.2119172  r2 : -0.39243782\n",
      "Epoch 225 loss : 0.9290006  val_loss : 1.2224411  r2 : -0.4045292\n",
      "Epoch 226 loss : 1.103652  val_loss : 1.3938429  r2 : -0.6014621\n",
      "Epoch 227 loss : 1.1369061  val_loss : 1.4368836  r2 : -0.65091383\n",
      "Epoch 228 loss : 1.4017891  val_loss : 1.7109954  r2 : -0.96585596\n",
      "Epoch 229 loss : 1.6334758  val_loss : 1.9381323  r2 : -1.226826\n",
      "Epoch 230 loss : 1.8743957  val_loss : 2.1271145  r2 : -1.4439578\n",
      "Epoch 231 loss : 2.144811  val_loss : 2.4411633  r2 : -1.8047857\n",
      "Epoch 232 loss : 2.1127057  val_loss : 2.4061818  r2 : -1.7645936\n",
      "Epoch 233 loss : 1.3694351  val_loss : 1.8274525  r2 : -1.0996599\n",
      "Epoch 234 loss : 0.7002242  val_loss : 1.2488439  r2 : -0.43486488\n",
      "Epoch 235 loss : 0.27518773  val_loss : 0.64519656  r2 : 0.25869846\n",
      "Epoch 236 loss : 0.28914046  val_loss : 0.4197739  r2 : 0.5176989\n",
      "Epoch 237 loss : 0.25721246  val_loss : 0.38631454  r2 : 0.5561421\n",
      "Epoch 238 loss : 0.20125364  val_loss : 0.34204763  r2 : 0.60700285\n",
      "Epoch 239 loss : 0.21937111  val_loss : 0.3468621  r2 : 0.6014712\n",
      "Epoch 240 loss : 0.2207447  val_loss : 0.3426104  r2 : 0.60635626\n",
      "Epoch 241 loss : 0.22711904  val_loss : 0.33573952  r2 : 0.61425054\n",
      "Epoch 242 loss : 0.21548776  val_loss : 0.32448614  r2 : 0.62718016\n",
      "Epoch 243 loss : 0.21954432  val_loss : 0.32423455  r2 : 0.6274693\n",
      "Epoch 244 loss : 0.22498861  val_loss : 0.3259151  r2 : 0.62553835\n",
      "Epoch 245 loss : 0.2197992  val_loss : 0.32364812  r2 : 0.628143\n",
      "Epoch 246 loss : 0.2267499  val_loss : 0.32120815  r2 : 0.63094646\n",
      "Epoch 247 loss : 0.22028968  val_loss : 0.3214402  r2 : 0.63067985\n",
      "Epoch 248 loss : 0.22016451  val_loss : 0.31786796  r2 : 0.63478416\n",
      "Epoch 249 loss : 0.22268471  val_loss : 0.32045043  r2 : 0.63181704\n",
      "Epoch 250 loss : 0.21962085  val_loss : 0.31756544  r2 : 0.6351317\n",
      "Epoch 251 loss : 0.22701998  val_loss : 0.319861  r2 : 0.6324942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 252 loss : 0.22382563  val_loss : 0.32206842  r2 : 0.62995803\n",
      "Epoch 253 loss : 0.22483419  val_loss : 0.32031327  r2 : 0.6319746\n",
      "Epoch 254 loss : 0.22577694  val_loss : 0.31753635  r2 : 0.63516515\n",
      "Epoch 255 loss : 0.22480494  val_loss : 0.31197333  r2 : 0.6415568\n",
      "Epoch 256 loss : 0.2263542  val_loss : 0.31602433  r2 : 0.63690245\n",
      "Epoch 257 loss : 0.22172338  val_loss : 0.308732  r2 : 0.64528096\n",
      "Epoch 258 loss : 0.22743966  val_loss : 0.312977  r2 : 0.6404036\n",
      "Epoch 259 loss : 0.22544834  val_loss : 0.30990884  r2 : 0.6439288\n",
      "Epoch 260 loss : 0.22130434  val_loss : 0.30975908  r2 : 0.6441009\n",
      "Epoch 261 loss : 0.23179142  val_loss : 0.3137778  r2 : 0.6394836\n",
      "Epoch 262 loss : 0.2316889  val_loss : 0.3165902  r2 : 0.6362523\n",
      "Epoch 263 loss : 0.23186195  val_loss : 0.31157243  r2 : 0.6420174\n",
      "Epoch 264 loss : 0.23034166  val_loss : 0.31416485  r2 : 0.63903886\n",
      "Epoch 265 loss : 0.2300437  val_loss : 0.3093597  r2 : 0.64455974\n",
      "Epoch 266 loss : 0.23483649  val_loss : 0.31537005  r2 : 0.6376541\n",
      "Epoch 267 loss : 0.2370642  val_loss : 0.31597355  r2 : 0.63696074\n",
      "Epoch 268 loss : 0.23576358  val_loss : 0.31565386  r2 : 0.637328\n",
      "Epoch 269 loss : 0.2334116  val_loss : 0.3134252  r2 : 0.63988864\n",
      "Epoch 270 loss : 0.23704281  val_loss : 0.31487548  r2 : 0.63822234\n",
      "Epoch 271 loss : 0.23676386  val_loss : 0.31826648  r2 : 0.6343263\n",
      "Epoch 272 loss : 0.23409708  val_loss : 0.3068512  r2 : 0.64744186\n",
      "Epoch 273 loss : 0.24368623  val_loss : 0.31801426  r2 : 0.6346161\n",
      "Epoch 274 loss : 0.2269124  val_loss : 0.3079434  r2 : 0.64618707\n",
      "Epoch 275 loss : 0.23765558  val_loss : 0.3155181  r2 : 0.637484\n",
      "Epoch 276 loss : 0.23626855  val_loss : 0.311668  r2 : 0.64190763\n",
      "Epoch 277 loss : 0.2507866  val_loss : 0.31679264  r2 : 0.63601965\n",
      "Epoch 278 loss : 0.25063333  val_loss : 0.3223613  r2 : 0.6296215\n",
      "Epoch 279 loss : 0.24207373  val_loss : 0.32380158  r2 : 0.6279667\n",
      "Epoch 280 loss : 0.24653603  val_loss : 0.32471782  r2 : 0.62691396\n",
      "Epoch 281 loss : 0.24847233  val_loss : 0.3240444  r2 : 0.6276877\n",
      "Epoch 282 loss : 0.2507747  val_loss : 0.32866377  r2 : 0.62238026\n",
      "Epoch 283 loss : 0.2492618  val_loss : 0.3356864  r2 : 0.6143116\n",
      "Epoch 284 loss : 0.22793986  val_loss : 0.324016  r2 : 0.62772036\n",
      "Epoch 285 loss : 0.2179  val_loss : 0.30621266  r2 : 0.6481756\n",
      "Epoch 286 loss : 0.21568568  val_loss : 0.3109339  r2 : 0.6427511\n",
      "Epoch 287 loss : 0.2293492  val_loss : 0.31839186  r2 : 0.6341822\n",
      "Epoch 288 loss : 0.22754139  val_loss : 0.31958574  r2 : 0.6328105\n",
      "Epoch 289 loss : 0.23512173  val_loss : 0.32045776  r2 : 0.63180864\n",
      "Epoch 290 loss : 0.21510682  val_loss : 0.31842044  r2 : 0.6341494\n",
      "Epoch 291 loss : 0.21763504  val_loss : 0.3043795  r2 : 0.6502818\n",
      "Epoch 292 loss : 0.21395369  val_loss : 0.31271613  r2 : 0.6407034\n",
      "Epoch 293 loss : 0.21675429  val_loss : 0.3090051  r2 : 0.6449672\n",
      "Epoch 294 loss : 0.21765816  val_loss : 0.30817  r2 : 0.6459267\n",
      "Epoch 295 loss : 0.22551046  val_loss : 0.31380716  r2 : 0.63944983\n",
      "Epoch 296 loss : 0.21469161  val_loss : 0.31366736  r2 : 0.6396104\n",
      "Epoch 297 loss : 0.21754846  val_loss : 0.30628866  r2 : 0.6480882\n",
      "Epoch 298 loss : 0.22597954  val_loss : 0.3161499  r2 : 0.63675815\n",
      "Epoch 299 loss : 0.1911482  val_loss : 0.2969602  r2 : 0.6588062\n",
      "Epoch 300 loss : 0.19425832  val_loss : 0.2824706  r2 : 0.67545414\n",
      "Epoch 301 loss : 0.1959934  val_loss : 0.296496  r2 : 0.65933955\n",
      "Epoch 302 loss : 0.20613627  val_loss : 0.29674807  r2 : 0.65905\n",
      "Epoch 303 loss : 0.22290877  val_loss : 0.30925336  r2 : 0.64468193\n",
      "Epoch 304 loss : 0.20719612  val_loss : 0.30916747  r2 : 0.64478064\n",
      "Epoch 305 loss : 0.21668312  val_loss : 0.30584258  r2 : 0.6486008\n",
      "Epoch 306 loss : 0.23082758  val_loss : 0.32629657  r2 : 0.6251001\n",
      "Epoch 307 loss : 0.22652858  val_loss : 0.33086076  r2 : 0.619856\n",
      "Epoch 308 loss : 0.21094553  val_loss : 0.31273398  r2 : 0.6406829\n",
      "Epoch 309 loss : 0.2145374  val_loss : 0.32101402  r2 : 0.6311695\n",
      "Epoch 310 loss : 0.20078222  val_loss : 0.2949306  r2 : 0.6611381\n",
      "Epoch 311 loss : 0.19079745  val_loss : 0.290294  r2 : 0.6664654\n",
      "Epoch 312 loss : 0.14194545  val_loss : 0.23863865  r2 : 0.72581506\n",
      "Epoch 313 loss : 0.15876774  val_loss : 0.25003257  r2 : 0.712724\n",
      "Epoch 314 loss : 0.16519538  val_loss : 0.26448506  r2 : 0.6961187\n",
      "Epoch 315 loss : 0.17932464  val_loss : 0.27066955  r2 : 0.689013\n",
      "Epoch 316 loss : 0.17462608  val_loss : 0.27624744  r2 : 0.68260425\n",
      "Epoch 317 loss : 0.15638681  val_loss : 0.23940276  r2 : 0.7249371\n",
      "Epoch 318 loss : 0.16389161  val_loss : 0.25754324  r2 : 0.7040945\n",
      "Epoch 319 loss : 0.1758875  val_loss : 0.27330765  r2 : 0.6859819\n",
      "Epoch 320 loss : 0.20452596  val_loss : 0.30113578  r2 : 0.6540086\n",
      "Epoch 321 loss : 0.22971763  val_loss : 0.33182907  r2 : 0.6187435\n",
      "Epoch 322 loss : 0.25251445  val_loss : 0.34909403  r2 : 0.5989068\n",
      "Epoch 323 loss : 0.27587497  val_loss : 0.37003836  r2 : 0.57484275\n",
      "Epoch 324 loss : 0.30696645  val_loss : 0.38445318  r2 : 0.55828077\n",
      "Epoch 325 loss : 0.3418202  val_loss : 0.41357052  r2 : 0.5248263\n",
      "Epoch 326 loss : 0.43209246  val_loss : 0.5422884  r2 : 0.37693524\n",
      "Epoch 327 loss : 0.43832597  val_loss : 0.5672325  r2 : 0.3482756\n",
      "Epoch 328 loss : 0.34699103  val_loss : 0.4940126  r2 : 0.43240196\n",
      "Epoch 329 loss : 0.29566136  val_loss : 0.40616602  r2 : 0.5333337\n",
      "Epoch 330 loss : 0.18079562  val_loss : 0.3342324  r2 : 0.6159822\n",
      "Epoch 331 loss : 0.16606297  val_loss : 0.30172962  r2 : 0.6533264\n",
      "Epoch 332 loss : 0.16574554  val_loss : 0.28230792  r2 : 0.67564106\n",
      "Epoch 333 loss : 0.15142408  val_loss : 0.28145957  r2 : 0.6766157\n",
      "Epoch 334 loss : 0.1518504  val_loss : 0.2793436  r2 : 0.67904687\n",
      "Epoch 335 loss : 0.15738504  val_loss : 0.26638573  r2 : 0.6939349\n",
      "Epoch 336 loss : 0.14778864  val_loss : 0.27480212  r2 : 0.68426484\n",
      "Epoch 337 loss : 0.14019525  val_loss : 0.27768666  r2 : 0.68095064\n",
      "Epoch 338 loss : 0.14585602  val_loss : 0.27319157  r2 : 0.68611526\n",
      "Epoch 339 loss : 0.16019762  val_loss : 0.2856043  r2 : 0.67185366\n",
      "Epoch 340 loss : 0.15918604  val_loss : 0.2973318  r2 : 0.65837926\n",
      "Epoch 341 loss : 0.1824272  val_loss : 0.30023816  r2 : 0.65504\n",
      "Epoch 342 loss : 0.17493032  val_loss : 0.3214747  r2 : 0.63064015\n",
      "Epoch 343 loss : 0.15408924  val_loss : 0.2813198  r2 : 0.6767763\n",
      "Epoch 344 loss : 0.16049697  val_loss : 0.28118864  r2 : 0.67692703\n",
      "Epoch 345 loss : 0.1509746  val_loss : 0.2915668  r2 : 0.665003\n",
      "Epoch 346 loss : 0.15084904  val_loss : 0.29415554  r2 : 0.6620287\n",
      "Epoch 347 loss : 0.16471502  val_loss : 0.29067266  r2 : 0.6660303\n",
      "Epoch 348 loss : 0.14349921  val_loss : 0.28228948  r2 : 0.67566216\n",
      "Epoch 349 loss : 0.15119804  val_loss : 0.28263137  r2 : 0.67526937\n",
      "Epoch 350 loss : 0.1453572  val_loss : 0.27187496  r2 : 0.68762803\n",
      "Epoch 351 loss : 0.13075553  val_loss : 0.2573506  r2 : 0.7043159\n",
      "Epoch 352 loss : 0.14339237  val_loss : 0.2512478  r2 : 0.7113277\n",
      "Epoch 353 loss : 0.15344767  val_loss : 0.2803782  r2 : 0.67785823\n",
      "Epoch 354 loss : 0.13608661  val_loss : 0.27537864  r2 : 0.68360245\n",
      "Epoch 355 loss : 0.17233159  val_loss : 0.28078574  r2 : 0.6773899\n",
      "Epoch 356 loss : 0.14690295  val_loss : 0.30139458  r2 : 0.6537113\n",
      "Epoch 357 loss : 0.179586  val_loss : 0.3158861  r2 : 0.63706124\n",
      "Epoch 358 loss : 0.16191867  val_loss : 0.29053378  r2 : 0.6661899\n",
      "Epoch 359 loss : 0.15892005  val_loss : 0.34115353  r2 : 0.6080301\n",
      "Epoch 360 loss : 0.1767223  val_loss : 0.31615195  r2 : 0.6367558\n",
      "Epoch 361 loss : 0.16254593  val_loss : 0.26744932  r2 : 0.6927129\n",
      "Epoch 362 loss : 0.18558003  val_loss : 0.28096342  r2 : 0.6771858\n",
      "Epoch 363 loss : 0.18340525  val_loss : 0.31419954  r2 : 0.638999\n",
      "Epoch 364 loss : 0.4648257  val_loss : 0.47738454  r2 : 0.45150685\n",
      "Epoch 365 loss : 0.2770969  val_loss : 0.38641137  r2 : 0.55603087\n",
      "Epoch 366 loss : 0.47505423  val_loss : 0.5087414  r2 : 0.41547924\n",
      "Epoch 367 loss : 0.7618403  val_loss : 0.84282243  r2 : 0.031635344\n",
      "Epoch 368 loss : 0.3134931  val_loss : 0.5181152  r2 : 0.40470916\n",
      "Epoch 369 loss : 0.1989528  val_loss : 0.2695073  r2 : 0.6903484\n",
      "Epoch 370 loss : 0.17381161  val_loss : 0.24726926  r2 : 0.7158989\n",
      "Epoch 371 loss : 0.19287348  val_loss : 0.26621583  r2 : 0.69413006\n",
      "Epoch 372 loss : 0.18117845  val_loss : 0.2653149  r2 : 0.6951652\n",
      "Epoch 373 loss : 0.15584171  val_loss : 0.24005832  r2 : 0.7241839\n",
      "Epoch 374 loss : 0.15684824  val_loss : 0.2368875  r2 : 0.727827\n",
      "Epoch 375 loss : 0.1493054  val_loss : 0.23325689  r2 : 0.73199844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 376 loss : 0.1407342  val_loss : 0.23103063  r2 : 0.7345563\n",
      "Epoch 377 loss : 0.13823923  val_loss : 0.23095389  r2 : 0.7346445\n",
      "Epoch 378 loss : 0.14146453  val_loss : 0.22880358  r2 : 0.7371151\n",
      "Epoch 379 loss : 0.13009684  val_loss : 0.22508875  r2 : 0.74138325\n",
      "Epoch 380 loss : 0.13623594  val_loss : 0.22467403  r2 : 0.7418598\n",
      "Epoch 381 loss : 0.15077603  val_loss : 0.26450148  r2 : 0.6960998\n",
      "Epoch 382 loss : 0.13938527  val_loss : 0.26585573  r2 : 0.69454384\n",
      "Epoch 383 loss : 0.13589114  val_loss : 0.23709457  r2 : 0.72758913\n",
      "Epoch 384 loss : 0.1259224  val_loss : 0.23047157  r2 : 0.7351986\n",
      "Epoch 385 loss : 0.13224503  val_loss : 0.22453842  r2 : 0.7420156\n",
      "Epoch 386 loss : 0.1299011  val_loss : 0.2241444  r2 : 0.74246824\n",
      "Epoch 387 loss : 0.13456841  val_loss : 0.2250791  r2 : 0.74139434\n",
      "Epoch 388 loss : 0.133426  val_loss : 0.23219264  r2 : 0.7332212\n",
      "Epoch 389 loss : 0.13478364  val_loss : 0.23001434  r2 : 0.735724\n",
      "Epoch 390 loss : 0.13932605  val_loss : 0.24260853  r2 : 0.7212538\n",
      "Epoch 391 loss : 0.15043843  val_loss : 0.23463495  r2 : 0.7304151\n",
      "Epoch 392 loss : 0.13608976  val_loss : 0.23291907  r2 : 0.7323866\n",
      "Epoch 393 loss : 0.13217777  val_loss : 0.23324779  r2 : 0.7320089\n",
      "Epoch 394 loss : 0.1349419  val_loss : 0.23163706  r2 : 0.73385954\n",
      "Epoch 395 loss : 0.13573334  val_loss : 0.24223675  r2 : 0.721681\n",
      "Epoch 396 loss : 0.13741232  val_loss : 0.23074816  r2 : 0.7348808\n",
      "Epoch 397 loss : 0.13443924  val_loss : 0.23036043  r2 : 0.7353263\n",
      "Epoch 398 loss : 0.13755469  val_loss : 0.22723228  r2 : 0.73892045\n",
      "Epoch 399 loss : 0.13223906  val_loss : 0.23460308  r2 : 0.7304517\n",
      "Epoch 400 loss : 0.13858736  val_loss : 0.23061793  r2 : 0.7350305\n",
      "Epoch 401 loss : 0.13564661  val_loss : 0.23445843  r2 : 0.7306179\n",
      "Epoch 402 loss : 0.13886486  val_loss : 0.2352565  r2 : 0.729701\n",
      "Epoch 403 loss : 0.14914122  val_loss : 0.23621261  r2 : 0.7286024\n",
      "Epoch 404 loss : 0.14223056  val_loss : 0.249374  r2 : 0.7134806\n",
      "Epoch 405 loss : 0.14563154  val_loss : 0.24829407  r2 : 0.71472144\n",
      "Epoch 406 loss : 0.13537146  val_loss : 0.2511024  r2 : 0.71149474\n",
      "Epoch 407 loss : 0.1405787  val_loss : 0.23635305  r2 : 0.7284411\n",
      "Epoch 408 loss : 0.13900161  val_loss : 0.24232842  r2 : 0.7215756\n",
      "Epoch 409 loss : 0.13989009  val_loss : 0.24005276  r2 : 0.7241903\n",
      "Epoch 410 loss : 0.1410696  val_loss : 0.24978447  r2 : 0.713009\n",
      "Epoch 411 loss : 0.13932247  val_loss : 0.23948772  r2 : 0.72483945\n",
      "Epoch 412 loss : 0.20826785  val_loss : 0.29113623  r2 : 0.66549766\n",
      "Epoch 413 loss : 0.21227694  val_loss : 0.28671244  r2 : 0.6705804\n",
      "Epoch 414 loss : 0.30923423  val_loss : 0.56130433  r2 : 0.3550868\n",
      "Epoch 415 loss : 0.311866  val_loss : 0.3671651  r2 : 0.57814395\n",
      "Epoch 416 loss : 0.3829856  val_loss : 0.40102464  r2 : 0.5392409\n",
      "Epoch 417 loss : 0.6005648  val_loss : 0.71984416  r2 : 0.17293179\n",
      "Epoch 418 loss : 0.1419023  val_loss : 0.26199564  r2 : 0.6989789\n",
      "Epoch 419 loss : 0.20482647  val_loss : 0.2733716  r2 : 0.68590844\n",
      "Epoch 420 loss : 0.17630681  val_loss : 0.2597535  r2 : 0.701555\n",
      "Epoch 421 loss : 0.15694097  val_loss : 0.2459184  r2 : 0.717451\n",
      "Epoch 422 loss : 0.158601  val_loss : 0.2334992  r2 : 0.73172003\n",
      "Epoch 423 loss : 0.15844755  val_loss : 0.23055826  r2 : 0.735099\n",
      "Epoch 424 loss : 0.15752022  val_loss : 0.23164746  r2 : 0.7338476\n",
      "Epoch 425 loss : 0.15526657  val_loss : 0.22888239  r2 : 0.73702455\n",
      "Epoch 426 loss : 0.15130603  val_loss : 0.23167649  r2 : 0.73381424\n",
      "Epoch 427 loss : 0.15144403  val_loss : 0.2377569  r2 : 0.7268281\n",
      "Epoch 428 loss : 0.14833012  val_loss : 0.22881553  r2 : 0.7371014\n",
      "Epoch 429 loss : 0.15079585  val_loss : 0.23599267  r2 : 0.72885513\n",
      "Epoch 430 loss : 0.14947538  val_loss : 0.23621914  r2 : 0.7285949\n",
      "Epoch 431 loss : 0.14844288  val_loss : 0.23799193  r2 : 0.7265581\n",
      "Epoch 432 loss : 0.14597745  val_loss : 0.23555264  r2 : 0.7293607\n",
      "Epoch 433 loss : 0.1509194  val_loss : 0.23362146  r2 : 0.73157954\n",
      "Epoch 434 loss : 0.14744411  val_loss : 0.23203902  r2 : 0.7333977\n",
      "Epoch 435 loss : 0.14751881  val_loss : 0.23309715  r2 : 0.73218197\n",
      "Epoch 436 loss : 0.14211  val_loss : 0.22621499  r2 : 0.7400893\n",
      "Epoch 437 loss : 0.14422181  val_loss : 0.22805588  r2 : 0.73797417\n",
      "Epoch 438 loss : 0.14279044  val_loss : 0.23474741  r2 : 0.7302859\n",
      "Epoch 439 loss : 0.14032501  val_loss : 0.23574848  r2 : 0.7291357\n",
      "Epoch 440 loss : 0.13857836  val_loss : 0.22822188  r2 : 0.73778343\n",
      "Epoch 441 loss : 0.1411897  val_loss : 0.22657757  r2 : 0.73967266\n",
      "Epoch 442 loss : 0.14191148  val_loss : 0.23395678  r2 : 0.73119426\n",
      "Epoch 443 loss : 0.13874345  val_loss : 0.23265226  r2 : 0.73269314\n",
      "Epoch 444 loss : 0.13485432  val_loss : 0.23520754  r2 : 0.7297572\n",
      "Epoch 445 loss : 0.13309482  val_loss : 0.24498528  r2 : 0.718523\n",
      "Epoch 446 loss : 0.13150603  val_loss : 0.2386879  r2 : 0.72575843\n",
      "Epoch 447 loss : 0.13536465  val_loss : 0.23477808  r2 : 0.73025066\n",
      "Epoch 448 loss : 0.13528669  val_loss : 0.23488612  r2 : 0.7301265\n",
      "Epoch 449 loss : 0.13036785  val_loss : 0.24262865  r2 : 0.72123075\n",
      "Epoch 450 loss : 0.13296176  val_loss : 0.23345344  r2 : 0.7317726\n",
      "Epoch 451 loss : 0.13361746  val_loss : 0.22875415  r2 : 0.7371719\n",
      "Epoch 452 loss : 0.13572104  val_loss : 0.24158725  r2 : 0.72242725\n",
      "Epoch 453 loss : 0.14031608  val_loss : 0.24401613  r2 : 0.71963656\n",
      "Epoch 454 loss : 0.13565502  val_loss : 0.2279342  r2 : 0.738114\n",
      "Epoch 455 loss : 0.1441963  val_loss : 0.2434231  r2 : 0.7203179\n",
      "Epoch 456 loss : 0.13567814  val_loss : 0.25432652  r2 : 0.7077904\n",
      "Epoch 457 loss : 0.1348811  val_loss : 0.22950144  r2 : 0.7363132\n",
      "Epoch 458 loss : 0.13598987  val_loss : 0.24075714  r2 : 0.723381\n",
      "Epoch 459 loss : 0.13749634  val_loss : 0.24552555  r2 : 0.7179023\n",
      "Epoch 460 loss : 0.1363905  val_loss : 0.25598395  r2 : 0.7058861\n",
      "Epoch 461 loss : 0.13484001  val_loss : 0.24579567  r2 : 0.71759194\n",
      "Epoch 462 loss : 0.1317867  val_loss : 0.23878735  r2 : 0.7256442\n",
      "Epoch 463 loss : 0.1337292  val_loss : 0.23619096  r2 : 0.7286273\n",
      "Epoch 464 loss : 0.14799239  val_loss : 0.22918919  r2 : 0.73667204\n",
      "Epoch 465 loss : 0.14192101  val_loss : 0.27119327  r2 : 0.68841124\n",
      "Epoch 466 loss : 0.20593107  val_loss : 0.24319889  r2 : 0.7205756\n",
      "Epoch 467 loss : 0.14707744  val_loss : 0.22504467  r2 : 0.74143386\n",
      "Epoch 468 loss : 0.13830364  val_loss : 0.2673473  r2 : 0.6928301\n",
      "Epoch 469 loss : 0.14983274  val_loss : 0.28434098  r2 : 0.67330515\n",
      "Epoch 470 loss : 0.23934634  val_loss : 0.3129378  r2 : 0.6404487\n",
      "Epoch 471 loss : 0.19782376  val_loss : 0.26109943  r2 : 0.70000863\n",
      "Epoch 472 loss : 0.24380636  val_loss : 0.27909735  r2 : 0.6793299\n",
      "Epoch 473 loss : 0.2169642  val_loss : 0.25569016  r2 : 0.7062236\n",
      "Epoch 474 loss : 0.21322852  val_loss : 0.24122863  r2 : 0.72283924\n",
      "Epoch 475 loss : 0.1955648  val_loss : 0.23667806  r2 : 0.72806764\n",
      "Epoch 476 loss : 0.16696887  val_loss : 0.22157384  r2 : 0.74542177\n",
      "Epoch 477 loss : 0.16368823  val_loss : 0.22007185  r2 : 0.74714744\n",
      "Epoch 478 loss : 0.17653264  val_loss : 0.22920972  r2 : 0.73664844\n",
      "Epoch 479 loss : 0.1750852  val_loss : 0.23220806  r2 : 0.73320353\n",
      "Epoch 480 loss : 0.18826167  val_loss : 0.23838188  r2 : 0.72611004\n",
      "Epoch 481 loss : 0.18378335  val_loss : 0.23035988  r2 : 0.73532695\n",
      "Epoch 482 loss : 0.20363651  val_loss : 0.24847874  r2 : 0.71450925\n",
      "Epoch 483 loss : 0.1948191  val_loss : 0.25324258  r2 : 0.70903575\n",
      "Epoch 484 loss : 0.20019934  val_loss : 0.2428859  r2 : 0.7209351\n",
      "Epoch 485 loss : 0.22642319  val_loss : 0.2625595  r2 : 0.69833106\n",
      "Epoch 486 loss : 0.2188291  val_loss : 0.25825325  r2 : 0.7032788\n",
      "Epoch 487 loss : 0.2137098  val_loss : 0.24261835  r2 : 0.72124255\n",
      "Epoch 488 loss : 0.19476855  val_loss : 0.23235752  r2 : 0.73303175\n",
      "Epoch 489 loss : 0.15635648  val_loss : 0.2138674  r2 : 0.7542761\n",
      "Epoch 490 loss : 0.18604338  val_loss : 0.22591878  r2 : 0.7404296\n",
      "Epoch 491 loss : 0.20569108  val_loss : 0.28264165  r2 : 0.67525756\n",
      "Epoch 492 loss : 0.23451567  val_loss : 0.28678  r2 : 0.6705028\n",
      "Epoch 493 loss : 0.23953539  val_loss : 0.28911602  r2 : 0.6678188\n",
      "Epoch 494 loss : 0.29728016  val_loss : 0.3196702  r2 : 0.63271344\n",
      "Epoch 495 loss : 0.23760268  val_loss : 0.27781552  r2 : 0.6808026\n",
      "Epoch 496 loss : 0.23105119  val_loss : 0.29897112  r2 : 0.65649575\n",
      "Epoch 497 loss : 0.5142414  val_loss : 0.45239347  r2 : 0.4802205\n",
      "Epoch 498 loss : 0.22526617  val_loss : 0.28662676  r2 : 0.67067885\n",
      "Epoch 499 loss : 0.20449243  val_loss : 0.2876415  r2 : 0.669513\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.07\n",
    "batch_size = 50\n",
    "num_epochs = 500\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# 計算グラフに渡す引数の形を決める\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# trainのミニバッチイテレータ\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    単純な3層ニューラルネットワーク\n",
    "    \"\"\"\n",
    "\n",
    "    # 重みとバイアスの宣言\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
    "    return layer_output\n",
    "\n",
    "# ネットワーク構造の読み込み                               \n",
    "logits = example_net(X)\n",
    "\n",
    "# 目的関数\n",
    "loss_op = tf.reduce_mean(tf.square(Y - logits))\n",
    "# 最適化手法\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "# variableの初期化\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.name_scope('r2'):\n",
    "    r2 = 1 - (tf.reduce_sum(tf.square(Y - logits)) / tf.reduce_sum(tf.square(Y - tf.reduce_mean(Y))))\n",
    "    tf.summary.scalar('r2', r2)\n",
    "\n",
    "# 計算グラフの実行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        # エポックごとにループ\n",
    "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
    "        total_loss = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # ミニバッチごとにループ\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss,train_r2= sess.run([loss_op,r2], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "#             total_loss += loss\n",
    "        total_loss /= total_batch\n",
    "        val_loss,train_r2= sess.run([loss_op,r2], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch\",epoch,\"loss :\",loss,\" val_loss :\",val_loss,\" r2 :\",train_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
